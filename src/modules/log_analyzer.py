"""
æ—¥å¿—åˆ†ææ¨¡å—
æä¾›æ—¥å¿—æ–‡ä»¶è§£æå’Œåˆ†æçš„å·¥å…·å‡½æ•°å’ŒLangChainå·¥å…·
"""

import re
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from langchain.tools import BaseTool
from pydantic import BaseModel, Field
from config.settings import get_config
from src.utils.logger import get_logger
from src.utils.helpers import parse_log_level, extract_error_patterns, safe_json_loads, truncate_string, run_command
from src.utils.exceptions import LogAnalysisError, FileOperationError
from pathlib import Path

logger = get_logger(__name__)


class LogAnalyzerConfig(BaseModel):
    """æ—¥å¿—åˆ†æé…ç½®æ¨¡å‹"""
    max_file_size: str = Field(default_factory=lambda: get_config('file_manager.max_file_size', '100MB'))
    error_patterns: List[str] = Field(default_factory=lambda: [
        'error', 'exception', 'failed', 'timeout', 'connection refused', 'permission denied'
    ])
    warning_patterns: List[str] = Field(default_factory=lambda: [
        'warning', 'deprecated', 'notice'
    ])


class LogAnalyzerTool(BaseTool):
    """æ—¥å¿—åˆ†æLangChainå·¥å…·"""
    
    name: str = "log_analyzer"
    description: str = (
        "ç”¨äºåˆ†ææ—¥å¿—æ–‡ä»¶çš„å·¥å…·ã€‚æ”¯æŒè§£ææœåŠ¡æ—¥å¿—ï¼Œæœç´¢é”™è¯¯æˆ–å¼‚å¸¸ï¼Œ"
        "è‡ªåŠ¨åˆ†ç±»æ—¥å¿—å†…å®¹å¹¶ç”ŸæˆæŠ¥å‘Šï¼Œç»Ÿè®¡æ—¥å¿—çº§åˆ«ç­‰æ“ä½œã€‚"
        "è¾“å…¥åº”ä¸ºå…·ä½“çš„æ—¥å¿—åˆ†æè¯·æ±‚ï¼Œå¦‚'åˆ†æ/var/log/nginx/error.logä¸­çš„é”™è¯¯'ã€"
        "'ç”Ÿæˆaccess.logçš„è®¿é—®ç»Ÿè®¡æŠ¥å‘Š'æˆ–'æœç´¢æ‰€æœ‰æ—¥å¿—ä¸­çš„æ•°æ®åº“è¿æ¥é”™è¯¯'"
    )
    args_schema: Optional[BaseModel] = None
    
    def _run(self, analysis_request: str) -> str:
        """
        æ‰§è¡Œæ—¥å¿—åˆ†ææ“ä½œ
        
        Args:
            analysis_request: æ—¥å¿—åˆ†æè¯·æ±‚æè¿°
            
        Returns:
            åˆ†æç»“æœ
        """
        try:
            request_lower = analysis_request.lower()
            
            if "åˆ†æ" in request_lower or "analyze" in request_lower:
                log_file = self._extract_log_file(analysis_request)
                if log_file:
                    if "é”™è¯¯" in request_lower or "error" in request_lower:
                        return self._analyze_errors(log_file)
                    elif "è­¦å‘Š" in request_lower or "warning" in request_lower:
                        return self._analyze_warnings(log_file)
                    elif "æŠ¥å‘Š" in request_lower or "report" in request_lower:
                        return self._generate_report(log_file)
                    else:
                        return self._analyze_log_file(log_file)
                else:
                    return "è¯·æŒ‡å®šæ—¥å¿—æ–‡ä»¶è·¯å¾„ã€‚"
            elif "æœç´¢" in request_lower or "search" in request_lower:
                keyword = self._extract_keyword(analysis_request)
                log_file = self._extract_log_file(analysis_request)
                if log_file and keyword:
                    return self._search_log(log_file, keyword)
                elif keyword:
                    return self._search_all_logs(keyword)
                else:
                    return "è¯·æŒ‡å®šæœç´¢å…³é”®è¯å’Œæ—¥å¿—æ–‡ä»¶ã€‚"
            elif "ç»Ÿè®¡" in request_lower or "statistics" in request_lower:
                log_file = self._extract_log_file(analysis_request)
                if log_file:
                    return self._get_log_statistics(log_file)
                else:
                    return "è¯·æŒ‡å®šæ—¥å¿—æ–‡ä»¶ã€‚"
            else:
                return (
                    "æ”¯æŒçš„æ—¥å¿—åˆ†ææ“ä½œ:\\n"
                    "- åˆ†ææ—¥å¿—æ–‡ä»¶ (æŒ‡å®šè·¯å¾„)\\n"
                    "- æœç´¢æ—¥å¿—ä¸­çš„ç‰¹å®šå…³é”®è¯ (æŒ‡å®šæ–‡ä»¶å’Œå…³é”®è¯)\\n"
                    "- ç”Ÿæˆæ—¥å¿—æŠ¥å‘Š (æŒ‡å®šæ–‡ä»¶)\\n"
                    "- ç»Ÿè®¡æ—¥å¿—çº§åˆ«åˆ†å¸ƒ (æŒ‡å®šæ–‡ä»¶)\\n"
                    "- åˆ†æé”™è¯¯/è­¦å‘Šæ—¥å¿— (æŒ‡å®šæ–‡ä»¶)\\n"
                    "ç¤ºä¾‹: 'åˆ†æ/var/log/nginx/error.logä¸­çš„é”™è¯¯'"
                )
                
        except Exception as e:
            logger.error(f"æ—¥å¿—åˆ†æå¤±è´¥: {e}")
            raise LogAnalysisError(f"æ—¥å¿—åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
    
    def _extract_log_file(self, request: str) -> Optional[str]:
        """ä»è¯·æ±‚ä¸­æå–æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
        # å¸¸è§æ—¥å¿—è·¯å¾„
        common_logs = [
            '/var/log/nginx/error.log', '/var/log/nginx/access.log',
            '/var/log/apache2/error.log', '/var/log/apache2/access.log',
            '/var/log/mysql/error.log', '/var/log/syslog',
            '/var/log/auth.log', '/var/log/kern.log'
        ]
        
        for log_path in common_logs:
            if log_path in request:
                return log_path
        
        # æå–è·¯å¾„ï¼ˆæœ€åä¸€ä¸ªè¯æˆ–å¼•å·å†…ï¼‰
        if '"' in request:
            return request.split('"')[1]
        words = request.split()
        if len(words) > 2:
            return words[-1]
        
        return None
    
    def _extract_keyword(self, request: str) -> Optional[str]:
        """ä»è¯·æ±‚ä¸­æå–å…³é”®è¯"""
        keywords = ['error', 'warning', 'failed', 'timeout', 'connection', 'permission']
        for keyword in keywords:
            if keyword in request.lower():
                return keyword
        
        # æå–æ“ä½œæè¿°ä¸­çš„å…³é”®è¯
        words = request.split()
        for word in words:
            if len(word) > 3 and word.lower() not in ['åˆ†æ', 'analyze', 'æ—¥å¿—', 'log', 'æ–‡ä»¶', 'file']:
                return word.lower()
        
        return None
    
    def _analyze_log_file(self, log_file: str) -> str:
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        try:
            if not self._validate_log_file(log_file):
                return f"æ— æ³•è®¿é—®æ—¥å¿—æ–‡ä»¶ '{log_file}' æˆ–æ–‡ä»¶å¤ªå¤§ã€‚"
            
            content = self._read_log_file(log_file)
            if not content:
                return f"æ—¥å¿—æ–‡ä»¶ '{log_file}' ä¸ºç©ºæˆ–æ— æ³•è¯»å–ã€‚"
            
            # åŸºæœ¬ç»Ÿè®¡
            stats = parse_log_level(content)
            errors = extract_error_patterns(content)
            
            config = LogAnalyzerConfig()
            max_size_bytes = self._parse_size(config.max_file_size)
            
            result = f"æ—¥å¿—æ–‡ä»¶ '{log_file}' åˆ†ææŠ¥å‘Š:\\n"
            result += "=" * 50 + "\n"
            result += f"æ–‡ä»¶å¤§å°: {self._get_file_size(log_file)}\n"
            result += f"è¡Œæ•°: {len(content.splitlines())}\n\n"
            
            result += "æ—¥å¿—çº§åˆ«ç»Ÿè®¡:\n"
            result += "-" * 20 + "\n"
            for level, count in stats.items():
                percentage = (count / sum(stats.values()) * 100) if sum(stats.values()) > 0 else 0
                result += f"{level}: {count} ({percentage:.1f}%)\n"
            
            if errors:
                result += f"\nå‘ç° {len(errors)} ä¸ªé”™è¯¯æ¨¡å¼:\n"
                result += "-" * 20 + "\n"
                for error in errors[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                    result += f"- {truncate_string(error, 80)}\n"
                if len(errors) > 10:
                    result += f"\n... è¿˜æœ‰ {len(errors) - 10} ä¸ªé”™è¯¯\n"
            else:
                result += "\nâœ… æœªå‘ç°æ˜æ˜¾çš„é”™è¯¯æ¨¡å¼ã€‚"
            
            # æœ€è¿‘10è¡Œæ—¥å¿—
            lines = content.splitlines()
            recent_logs = '\n'.join(lines[-10:])
            result += f"\næœ€è¿‘10è¡Œæ—¥å¿—:\n"
            result += "-" * 20 + "\n"
            result += truncate_string(recent_logs, 1000)
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ†ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            raise LogAnalysisError(f"æ— æ³•åˆ†ææ—¥å¿—æ–‡ä»¶ '{log_file}': {str(e)}")
    
    def _analyze_errors(self, log_file: str) -> str:
        """åˆ†æé”™è¯¯æ—¥å¿—"""
        try:
            if not self._validate_log_file(log_file):
                return f"æ— æ³•è®¿é—®æ—¥å¿—æ–‡ä»¶ '{log_file}'ã€‚"
            
            content = self._read_log_file(log_file)
            errors = extract_error_patterns(content)
            
            if not errors:
                return f"âœ… åœ¨æ—¥å¿—æ–‡ä»¶ '{log_file}' ä¸­æœªå‘ç°é”™è¯¯ã€‚"
            
            # åˆ†ç±»é”™è¯¯
            error_types = {
                'è¿æ¥é”™è¯¯': 0,
                'æƒé™é”™è¯¯': 0,
                'è¶…æ—¶é”™è¯¯': 0,
                'æ–‡ä»¶é”™è¯¯': 0,
                'å…¶ä»–é”™è¯¯': 0
            }
            
            for error in errors:
                error_lower = error.lower()
                if any(keyword in error_lower for keyword in ['connection', 'refused', 'timeout']):
                    error_types['è¿æ¥é”™è¯¯'] += 1
                elif any(keyword in error_lower for keyword in ['permission', 'access denied']):
                    error_types['æƒé™é”™è¯¯'] += 1
                elif 'timeout' in error_lower:
                    error_types['è¶…æ—¶é”™è¯¯'] += 1
                elif any(keyword in error_lower for keyword in ['file', 'no such', 'cannot open']):
                    error_types['æ–‡ä»¶é”™è¯¯'] += 1
                else:
                    error_types['å…¶ä»–é”™è¯¯'] += 1
            
            result = f"é”™è¯¯åˆ†ææŠ¥å‘Š - '{log_file}':\n"
            result += "=" * 40 + "\n"
            result += f"æ€»é”™è¯¯æ•°: {len(errors)}\n\n"
            
            result += "é”™è¯¯ç±»å‹åˆ†å¸ƒ:\n"
            result += "-" * 15 + "\n"
            for error_type, count in error_types.items():
                if count > 0:
                    percentage = (count / len(errors) * 100)
                    result += f"{error_type}: {count} ({percentage:.1f}%)\n"
            
            result += "\nå…¸å‹é”™è¯¯ç¤ºä¾‹ (å‰5ä¸ª):\n"
            result += "-" * 20 + "\n"
            for error in errors[:5]:
                result += f"- {truncate_string(error, 100)}\n"
            
            if len(errors) > 5:
                result += f"\n... è¿˜æœ‰ {len(errors) - 5} ä¸ªé”™è¯¯"
            
            # å»ºè®®
            result += "\n\nğŸ’¡ ä¿®å¤å»ºè®®:\n"
            if error_types['è¿æ¥é”™è¯¯'] > 0:
                result += "- æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®\n"
            if error_types['æƒé™é”™è¯¯'] > 0:
                result += "- æ£€æŸ¥æ–‡ä»¶å’Œç›®å½•æƒé™\n"
            if error_types['è¶…æ—¶é”™è¯¯'] > 0:
                result += "- å¢åŠ è¶…æ—¶æ—¶é—´æˆ–ä¼˜åŒ–æ€§èƒ½\n"
            if error_types['æ–‡ä»¶é”™è¯¯'] > 0:
                result += "- éªŒè¯æ–‡ä»¶è·¯å¾„å’Œç£ç›˜ç©ºé—´\n"
            
            return result
            
        except Exception as e:
            logger.error(f"é”™è¯¯åˆ†æå¤±è´¥: {e}")
            raise LogAnalysisError(f"æ— æ³•åˆ†æé”™è¯¯æ—¥å¿— '{log_file}': {str(e)}")
    
    def _analyze_warnings(self, log_file: str) -> str:
        """åˆ†æè­¦å‘Šæ—¥å¿—"""
        try:
            if not self._validate_log_file(log_file):
                return f"æ— æ³•è®¿é—®æ—¥å¿—æ–‡ä»¶ '{log_file}'ã€‚"
            
            content = self._read_log_file(log_file)
            
            config = LogAnalyzerConfig()
            warning_count = sum(1 for line in content.splitlines() if any(pattern in line.lower() for pattern in config.warning_patterns))
            
            warnings = [line for line in content.splitlines() if any(pattern in line.lower() for pattern in config.warning_patterns)]
            
            result = f"è­¦å‘Šåˆ†ææŠ¥å‘Š - '{log_file}':\n"
            result += "=" * 40 + "\n"
            result += f"æ€»è­¦å‘Šæ•°: {warning_count}\n\n"
            
            if warnings:
                result += "æœ€è¿‘10ä¸ªè­¦å‘Š:\n"
                result += "-" * 15 + "\n"
                for warning in warnings[-10:]:
                    result += f"- {truncate_string(warning, 100)}\n"
            else:
                result += "âœ… æœªå‘ç°è­¦å‘Šæ—¥å¿—ã€‚"
            
            return result
            
        except Exception as e:
            logger.error(f"è­¦å‘Šåˆ†æå¤±è´¥: {e}")
            raise LogAnalysisError(f"æ— æ³•åˆ†æè­¦å‘Šæ—¥å¿— '{log_file}': {str(e)}")
    
    def _generate_report(self, log_file: str) -> str:
        """ç”Ÿæˆæ—¥å¿—æŠ¥å‘Š"""
        try:
            if not self._validate_log_file(log_file):
                return f"æ— æ³•è®¿é—®æ—¥å¿—æ–‡ä»¶ '{log_file}'ã€‚"
            
            content = self._read_log_file(log_file)
            stats = parse_log_level(content)
            errors = extract_error_patterns(content)
            
            total_lines = len(content.splitlines())
            total_errors = len(errors)
            
            result = f"æ—¥å¿—åˆ†ææŠ¥å‘Š - '{log_file}'\n"
            result += "=" * 50 + "\n"
            result += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            result += f"æ–‡ä»¶å¤§å°: {self._get_file_size(log_file)}\n"
            result += f"æ€»è¡Œæ•°: {total_lines}\n\n"
            
            result += "1. æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:\n"
            result += "-" * 20 + "\n"
            for level, count in stats.items():
                percentage = (count / total_lines * 100) if total_lines > 0 else 0
                result += f"{level}: {count} è¡Œ ({percentage:.1f}%)\n"
            
            result += f"\n2. é”™è¯¯ç»Ÿè®¡: {total_errors} ä¸ª\n"
            if errors:
                result += "é”™è¯¯æ¨¡å¼æ‘˜è¦:\n"
                error_summary = {}
                for error in errors:
                    key = error.split(':')[0].lower() if ':' in error else 'å…¶ä»–'
                    error_summary[key] = error_summary.get(key, 0) + 1
                
                for error_type, count in sorted(error_summary.items(), key=lambda x: x[1], reverse=True)[:5]:
                    result += f"  - {error_type}: {count} æ¬¡\n"
            
            result += "\n3. å¥åº·çŠ¶æ€:\n"
            result += "-" * 15 + "\n"
            if total_errors == 0:
                result += "ğŸŸ¢ å¥åº· - æœªå‘ç°é”™è¯¯\n"
            elif total_errors < 10:
                result += "ğŸŸ¡ è­¦å‘Š - å‘ç°å°‘é‡é”™è¯¯\n"
            else:
                result += "ğŸ”´ é—®é¢˜ - å‘ç°å¤§é‡é”™è¯¯ï¼Œéœ€è¦å…³æ³¨\n"
            
            # æ€§èƒ½æŒ‡æ ‡
            result += f"\n4. æ€§èƒ½æŒ‡æ ‡:\n"
            result += f"   - ERRORç‡: {(total_errors / total_lines * 100):.2f}%\n"
            result += f"   - æœ€è¿‘1å°æ—¶æ—¥å¿—è¡Œæ•°: {self._count_recent_logs(content, hours=1)}\n"
            
            return result
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            raise LogAnalysisError(f"æ— æ³•ç”Ÿæˆæ—¥å¿—æŠ¥å‘Š '{log_file}': {str(e)}")
    
    def _search_log(self, log_file: str, keyword: str) -> str:
        """æœç´¢æ—¥å¿—ä¸­çš„å…³é”®è¯"""
        try:
            if not self._validate_log_file(log_file):
                return f"æ— æ³•è®¿é—®æ—¥å¿—æ–‡ä»¶ '{log_file}'ã€‚"
            
            content = self._read_log_file(log_file)
            lines = content.splitlines()
            
            matches = []
            for i, line in enumerate(lines, 1):
                if keyword.lower() in line.lower():
                    matches.append((i, line.strip()))
            
            if not matches:
                return f"åœ¨ '{log_file}' ä¸­æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„æ—¥å¿—ã€‚"
            
            result = f"æœç´¢ç»“æœ - '{log_file}' (å…³é”®è¯: '{keyword}'):\n"
            result += "=" * 50 + "\n"
            result += f"æ‰¾åˆ° {len(matches)} å¤„åŒ¹é…:\n\n"
            
            for line_num, line in matches[:20]:  # æ˜¾ç¤ºå‰20ä¸ªåŒ¹é…
                result += f"ç¬¬ {line_num} è¡Œ: {truncate_string(line, 120)}\n"
            
            if len(matches) > 20:
                result += f"\n... è¿˜æœ‰ {len(matches) - 20} å¤„åŒ¹é…"
            
            return result
            
        except Exception as e:
            logger.error(f"æœç´¢æ—¥å¿—å¤±è´¥: {e}")
            raise LogAnalysisError(f"æ— æ³•æœç´¢æ—¥å¿— '{log_file}': {str(e)}")
    
    def _search_all_logs(self, keyword: str) -> str:
        """æœç´¢æ‰€æœ‰å¸¸è§æ—¥å¿—æ–‡ä»¶"""
        common_logs = [
            '/var/log/syslog', '/var/log/messages', '/var/log/auth.log',
            '/var/log/nginx/error.log', '/var/log/nginx/access.log',
            '/var/log/apache2/error.log', '/var/log/mysql/error.log'
        ]
        
        results = []
        for log_file in common_logs:
            if Path(log_file).exists():
                try:
                    content = self._read_log_file(log_file)
                    lines = content.splitlines()
                    matches = sum(1 for line in lines if keyword.lower() in line.lower())
                    if matches > 0:
                        results.append(f"{log_file}: {matches} å¤„åŒ¹é…")
                except Exception:
                    continue
        
        if not results:
            return f"æœªåœ¨å¸¸è§æ—¥å¿—æ–‡ä»¶ä¸­æ‰¾åˆ°åŒ…å« '{keyword}' çš„è®°å½•ã€‚"
        
        result = f"è·¨æ—¥å¿—æœç´¢ç»“æœ (å…³é”®è¯: '{keyword}'):\n"
        result += "=" * 40 + "\n"
        for res in results:
            result += f"- {res}\n"
        
        return result
    
    def _get_log_statistics(self, log_file: str) -> str:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not self._validate_log_file(log_file):
                return f"æ— æ³•è®¿é—®æ—¥å¿—æ–‡ä»¶ '{log_file}'ã€‚"
            
            content = self._read_log_file(log_file)
            stats = parse_log_level(content)
            
            total_lines = len(content.splitlines())
            
            result = f"æ—¥å¿—ç»Ÿè®¡ - '{log_file}':\n"
            result += "=" * 30 + "\n"
            result += f"æ€»è¡Œæ•°: {total_lines}\n\n"
            
            result += "æŒ‰çº§åˆ«ç»Ÿè®¡:\n"
            result += "-" * 15 + "\n"
            for level, count in stats.items():
                percentage = (count / total_lines * 100) if total_lines > 0 else 0
                result += f"{level}: {count} ({percentage:.1f}%)\n"
            
            # æ—¶é—´åˆ†å¸ƒï¼ˆå¦‚æœæ—¥å¿—æœ‰æ—¶é—´æˆ³ï¼‰
            time_pattern = r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})'
            timestamps = re.findall(time_pattern, content)
            if timestamps:
                recent_hour = sum(1 for ts in timestamps if datetime.now() - datetime.fromisoformat(ts.replace(' ', 'T')) < timedelta(hours=1))
                result += f"\næœ€è¿‘1å°æ—¶æ—¥å¿—: {recent_hour} è¡Œ ({recent_hour / total_lines * 100:.1f}%)"
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {e}")
            raise LogAnalysisError(f"æ— æ³•è·å–æ—¥å¿—ç»Ÿè®¡ '{log_file}': {str(e)}")
    
    def _validate_log_file(self, log_file: str) -> bool:
        """éªŒè¯æ—¥å¿—æ–‡ä»¶"""
        path = Path(log_file)
        if not path.exists():
            return False
        
        if not path.is_file():
            return False
        
        config = LogAnalyzerConfig()
        max_size_bytes = self._parse_size(config.max_file_size)
        
        if path.stat().st_size > max_size_bytes:
            logger.warning(f"æ—¥å¿—æ–‡ä»¶ '{log_file}' å¤ªå¤§ ({path.stat().st_size} > {max_size_bytes})")
            return False
        
        return True
    
    def _read_log_file(self, log_file: str, max_lines: int = 10000) -> str:
        """è¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆé™åˆ¶è¡Œæ•°ï¼‰"""
        try:
            path = Path(log_file)
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()[-max_lines:]  # è¯»å–æœ€åmax_linesè¡Œ
                return ''.join(lines)
        except Exception as e:
            logger.error(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            raise FileOperationError(f"æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶ '{log_file}': {str(e)}")
    
    def _get_file_size(self, log_file: str) -> str:
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            path = Path(log_file)
            return f"{path.stat().st_size / 1024 / 1024:.2f} MB"
        except:
            return "æœªçŸ¥"
    
    def _parse_size(self, size_str: str) -> int:
        """è§£æå¤§å°å­—ç¬¦ä¸²"""
        size_str = size_str.upper().strip()
        if size_str.endswith('KB'):
            return int(float(size_str[:-2]) * 1024)
        elif size_str.endswith('MB'):
            return int(float(size_str[:-2]) * 1024 * 1024)
        elif size_str.endswith('GB'):
            return int(float(size_str[:-2]) * 1024 * 1024 * 1024)
        else:
            return int(size_str)
    
    def _count_recent_logs(self, content: str, hours: int = 1) -> int:
        """ç»Ÿè®¡æœ€è¿‘hourså°æ—¶çš„æ—¥å¿—è¡Œæ•°"""
        time_pattern = r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})'
        timestamps = re.findall(time_pattern, content)
        
        now = datetime.now()
        recent_count = 0
        
        for ts_str in timestamps:
            try:
                ts = datetime.fromisoformat(ts_str.replace(' ', 'T'))
                if now - ts < timedelta(hours=hours):
                    recent_count += 1
            except ValueError:
                continue
        
        return recent_count


if __name__ == "__main__":
    # æµ‹è¯•æ—¥å¿—åˆ†æå·¥å…·
    try:
        tool = LogAnalyzerTool()
        print("æµ‹è¯•æ—¥å¿—åˆ†æå·¥å…·:")
        # ç”±äºæ²¡æœ‰å®é™…æ—¥å¿—æ–‡ä»¶ï¼Œè¿™é‡Œæµ‹è¯•é€šç”¨åˆ†æ
        print(tool._run("åˆ†ææ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯"))
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
